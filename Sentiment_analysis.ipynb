{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation and Imports"
      ],
      "metadata": {
        "id": "CfkjBgNOQthp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55VL8iN050uV"
      },
      "source": [
        "The first thing I'm goint to do is to install the packages that i I need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wJmmQPs58ZI",
        "outputId": "8f74e9d2-f89c-4b17-c8fd-b8fafad88e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-self-attention in /Users/daniele/opt/anaconda3/lib/python3.9/site-packages (0.51.0)\n",
            "Requirement already satisfied: numpy in /Users/daniele/opt/anaconda3/lib/python3.9/site-packages (from keras-self-attention) (1.20.3)\n",
            "Requirement already satisfied: tensorflow_addons in /Users/daniele/opt/anaconda3/lib/python3.9/site-packages (0.18.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /Users/daniele/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_addons) (2.13.3)\n",
            "Requirement already satisfied: packaging in /Users/daniele/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_addons) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Users/daniele/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow_addons) (3.0.4)\n",
            "Requirement already satisfied: wget in /Users/daniele/opt/anaconda3/lib/python3.9/site-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-self-attention\n",
        "!pip install tensorflow_addons\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ69mRvf6NAM"
      },
      "source": [
        "Then I Import all the main libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N_3bvAm6J1t"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wget\n",
        "import wget\n",
        "import nltk\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3ugQlMj64oW"
      },
      "source": [
        "# Import of the Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPeGwzQc63kY"
      },
      "outputs": [],
      "source": [
        "test_set= pd.read_csv('test_ekmann.csv')\n",
        "train_set= pd.read_csv('train_ekmann.csv')\n",
        "val_set= pd.read_csv('val_ekmann.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "185-pOpd6_KC"
      },
      "source": [
        "# Pre-Processing\n",
        "\n",
        "In this case we have in input text sentences.\n",
        "\n",
        "I adopted a basic strategy, so in order to clean as much as I could the dataset, I did the following operations:\n",
        "\n",
        "- I edited everything in lower case\n",
        "- I removed all the emoji \n",
        "- I removed the punctuation (except question marks and exclamation points)\n",
        "- I replaced the question marks with the word \"question\" and the exclamation points with \"esclamation\"\n",
        "- I removed all the numbers\n",
        "- I removed all the stopwords \n",
        "\n",
        "Initially I also tried using stemming and lemming techniques but they worsened the performance of the model, so I removed them"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess of text"
      ],
      "metadata": {
        "id": "letEhfzgRQt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df:pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "  '''\n",
        "  Preprocess function for the input text, it takes\n",
        "  in input the original dataframe and manipulate it \n",
        "  into a cleaner dataframe\n",
        "\n",
        "  input: original dataframe\n",
        "  output: processed dataframe\n",
        "  '''\n",
        "\n",
        "  # edit in lower case\n",
        "  df['Text']=df['Text'].str.lower()\n",
        "\n",
        "  # remove emoji\n",
        "  emoji_filter = lambda c: ord(c) < 256\n",
        "  df['Text'] = df['Text'].apply(lambda s: ''.join(filter(emoji_filter, s)))\n",
        "\n",
        "  # remove punctuation (except \"?\" and \"!\" )\n",
        "  train_set['Text'] = train_set['Text'].str.replace(r'[^\\w\\s\\?\\!]+', '')\n",
        "\n",
        "  # replace \"?\" with 'question' and \"!\" with 'esclamation'\n",
        "  df['Text'] = df['Text'].str.replace('?',' question')\n",
        "  df['Text'] = df['Text'].str.replace('!',' esclamation') \n",
        "\n",
        "  # remove numbers\n",
        "  df['Text'] = df['Text'].str.replace('\\d+', '')\n",
        "\n",
        "  # remove stopwords\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "zpVwJkn_RtlZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df=preprocess_text(train_df)\n",
        "val_df=preprocess_text(val_df)\n",
        "test_df=preprocess_text(test_df)"
      ],
      "metadata": {
        "id": "cPDooPplTaWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79GF1kpPDWpH"
      },
      "source": [
        "## Preprocessing of labels\n",
        "\n",
        "For the labels (the \"Emotion\" column of the dataframe) I just applied the LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82n-vnPWDWNR"
      },
      "outputs": [],
      "source": [
        "def using_to_categorical(doc):\n",
        "    label_encoder = LabelEncoder()\n",
        "    data = label_encoder.fit_transform(doc)\n",
        "    data = np.array(data)\n",
        "    encoded = to_categorical(data)\n",
        "    return encoded"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=using_to_categorical(train_set['Emotion'])\n",
        "y_val=using_to_categorical(val_set['Emotion'])\n",
        "y_test=using_to_categorical(test_set['Emotion'])"
      ],
      "metadata": {
        "id": "F5UhvuF9Tk7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er6W-1NDEavI"
      },
      "source": [
        "# Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fXlC-n3AEjY"
      },
      "source": [
        "For my model I decided to use a pre-trained embedded layer\n",
        "\n",
        "I used an unsupervised learning algorithm for obtaining vector representations for words called GloVe\n",
        "\n",
        "This dataset contains English word vectors pre-trained and there are from 25 up to 200 dimensional pre trained word vectors. I decided to use the 200 dimentional word vector\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwVisPqcFHhv"
      },
      "source": [
        "First I have to use a tokenizer to the sentences in order to separate all the words and assign them a number\n",
        "\n",
        "Then I use pad_sequences to make all sentences the same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHa13k8GFEIU"
      },
      "outputs": [],
      "source": [
        "max_length=200 #I choose 200 because as we will see later I will use a dataset containing 200-dimention word vectors\n",
        "\n",
        "train_sentences=np.asarray(train_set['Text'])\n",
        "val_sentences=np.asarray(val_set['Text'])\n",
        "test_sentences=np.asarray(test_set['Text'])\n",
        "\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "vocab_size=len(tokenizer.word_index)+1\n",
        "\n",
        "train_encoded_document=tokenizer.texts_to_sequences(train_sentences)\n",
        "x_train=pad_sequences(train_encoded_document,maxlen=max_length,padding='pre')\n",
        "\n",
        "val_encoded_document=tokenizer.texts_to_sequences(val_sentences)\n",
        "x_val=pad_sequences(val_encoded_document,maxlen=max_length,padding='pre')\n",
        "\n",
        "\n",
        "test_encoded_document=tokenizer.texts_to_sequences(test_sentences)\n",
        "x_test=pad_sequences(test_encoded_document,maxlen=max_length,padding='pre')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onWhFN1OBe-0"
      },
      "source": [
        "Download of the dataset for the word embedding "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fw5pLmkbAFCG",
        "outputId": "eef57f93-f79b-4e33-b5e2-bd55eaca2bb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-06-30 12:21:32--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2022-06-30 12:21:33--  https://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  5.12MB/s    in 4m 47s  \n",
            "\n",
            "2022-06-30 12:26:20 (5.05 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "l!unzip -q glove.twitter.27B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIUbxc0lB9CM"
      },
      "source": [
        "Load all the word vectors in it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obDyDdyOB86J",
        "outputId": "e23e2df8-817e-4622-9b58-c3492c782260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leaded word vectors =  1193514\n"
          ]
        }
      ],
      "source": [
        "embedding_index=dict()\n",
        "f=open('glove.twitter.27B.200d.txt')\n",
        "for line in f:\n",
        "  values=line.split()\n",
        "  word=values[0]\n",
        "  coefs=np.asarray(values[1:],dtype='float32')\n",
        "  embedding_index[word]=coefs\n",
        "f.close()\n",
        "print('Leaded word vectors = ', len(embedding_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i4Y0tWJCRKN"
      },
      "source": [
        "Create the embedding_matrix that contains the pre-trained weigths for the embedded layer that I will use in my model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piNtTdKRCRDu",
        "outputId": "3aba53b2-a36a-4c37-8b26-b2b3b5671375"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(27161, 5432200)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix=np.zeros((vocab_size,max_length))\n",
        "for word,i in tokenizer.word_index.items():\n",
        "  embedding_vector=embedding_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i]=embedding_vector\n",
        "\n",
        "len(embedding_matrix),embedding_matrix.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd6t-LNUGjZa"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7MTCJebbDvs"
      },
      "source": [
        "First I import the model and all the layers I'm going to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbqP3fOhbDO5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import MaxPool1D\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import GlobalMaxPool1D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from keras_self_attention import SeqSelfAttention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-7WtO-6dQhf"
      },
      "source": [
        "## Implementation of the Model\n",
        "\n",
        "For the implementation of the neural nerwork I used 15 layers with a total number of about 6 milion of parameters\n",
        "\n",
        "- First I added the **Input layer**\n",
        "\n",
        "- After the input layer I inserted the pre-trained **Embedded layer**; in input of this layer I specified the size of the vocabolary, the length of the sentences and the embedding_matrix (containing the pre-trained weigths)\n",
        "\n",
        "- Then I decided to insert two **Bidirectional layers**, as instance I used the LSTM layer(I also tried with GRU layer but LSTM worked better). I inserted two of these layer because I saw that it worked very well on my model\n",
        "\n",
        "- I also added an **Attention Layer**, more precisely the SeqSelfAttention layer (I also tried the keras Attention layer but I had many problem using it and place it correctly in the model). After some researches I have read a paper (\"Attention is all you need\"[1]) emphasised the importance of the attention layer in a NLP network so I implemented it\n",
        "\n",
        "- After the attention layer I added:\n",
        " - A **Convolutional Layer** (it creates a convolution kernel that is convolved with the layer input over a single spatial dimension to produce a tensor of outputs). It has been usefull to connect efficiently the attention layer with the Max Pooling Layer\n",
        " - A **Max Pooling Layer 1D** (in order to apply the Max pooling operation)\n",
        " - A **DropOut Layer** (helps to prevent overfitting)\n",
        " - A **Concatenate Layer** (I concatenated the Bidirectional and the Attention Layer in order to consider the whole context and calculate the relevance)\n",
        " - A **Global Max Pooling Layer**\n",
        "- The configuration of the last 4 layers was inspired by a NLP network implemented by Polignano, M., Basile, P., de Gemmis, M., & Semeraro, G. in 2019 in their paper [2]. Expecially the Concatenate layer proved to be extremely high-performance in the model\n",
        "- Finally I added some **Dense** layers that increased the performance. The number of dense layer and neuron for each layer has been choosen after several trials. With fewer parameters the model performed worse, while with higher parameters the model overfitted very soon. This is the combination that gave me the best results.\n",
        "\n",
        "\n",
        "I did several attempt with many different layers and layer configuration. After all I reached the best score (I took as a reference point the f1_score) using the following configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN9gc6jSWXvC"
      },
      "outputs": [],
      "source": [
        "input_layer = Input((max_length))\n",
        "\n",
        "l_1 = Embedding( vocab_size , max_length , weights = [embedding_matrix] , input_length = max_length , trainable=False)(input_layer)\n",
        "l_2 = Bidirectional(LSTM(32,return_sequences=True,dropout=0.3, activation='tanh'))(l_1)\n",
        "l_3 = Bidirectional(LSTM(32,return_sequences=True,dropout=0.3, activation='tanh'))(l_2)\n",
        "l_4 = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                        kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                        bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                        attention_regularizer_weight=1e-4,\n",
        "                        attention_width=15,\n",
        "                        name='Attention')(l_3)\n",
        "\n",
        "l_5 = Conv1D(512, 3, activation='relu')(l_4)\n",
        "l_6 = MaxPool1D()(l_5)\n",
        "l_7 = Dropout(0.2)(l_6)\n",
        "l_8 = Concatenate(axis=1)([l_3,l_4])\n",
        "l_9 = GlobalMaxPool1D()(l_8)\n",
        "l_10 = Dense(1024,activation='relu')(l_9)\n",
        "l_11 = Dropout(0.2)(l_10)\n",
        "l_12 = Dense(512,activation='relu')(l_11)\n",
        "l_13 = Dropout(0.2)(l_12)\n",
        "l_14 = Dense(128,activation='relu')(l_13)\n",
        "l_15 = Dense(7,activation='softmax')(l_14)\n",
        "\n",
        "model = Model(inputs = input_layer, outputs = l_15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWKdDMKbiUwt",
        "outputId": "63d572a9-c2e4-4a2b-8b86-b6c142fd2cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_25 (Embedding)       (None, 200, 200)     5432200     ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " bidirectional_51 (Bidirectiona  (None, 200, 64)     59648       ['embedding_25[0][0]']           \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " bidirectional_52 (Bidirectiona  (None, 200, 64)     24832       ['bidirectional_51[0][0]']       \n",
            " l)                                                                                               \n",
            "                                                                                                  \n",
            " Attention (SeqSelfAttention)   (None, 200, 64)      4097        ['bidirectional_52[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 400, 64)      0           ['bidirectional_52[0][0]',       \n",
            "                                                                  'Attention[0][0]']              \n",
            "                                                                                                  \n",
            " global_max_pooling1d_16 (Globa  (None, 64)          0           ['concatenate_8[0][0]']          \n",
            " lMaxPooling1D)                                                                                   \n",
            "                                                                                                  \n",
            " dense_124 (Dense)              (None, 1024)         66560       ['global_max_pooling1d_16[0][0]']\n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)           (None, 1024)         0           ['dense_124[0][0]']              \n",
            "                                                                                                  \n",
            " dense_125 (Dense)              (None, 512)          524800      ['dropout_92[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)           (None, 512)          0           ['dense_125[0][0]']              \n",
            "                                                                                                  \n",
            " dense_126 (Dense)              (None, 128)          65664       ['dropout_93[0][0]']             \n",
            "                                                                                                  \n",
            " dense_127 (Dense)              (None, 7)            903         ['dense_126[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 6,178,704\n",
            "Trainable params: 746,504\n",
            "Non-trainable params: 5,432,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gih5e1_IeCr3"
      },
      "source": [
        "## Model Compilation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGe5UQDpeKeT"
      },
      "source": [
        "For the compilation I tried to use the fine tuning technique on the pre-trained embedded layer, so I compiled and trained the model for 2 epochs (with the pre-trained embedded layer setted trainable), then I saved the weigths.Finally I setted the layer non-trainable, I loaded the new weights and I compiled and trained again. Unfortunately this thecnique worsened the performance of my model so I decided to remove it and I opted for a basic compilation\n",
        "\n",
        "I used the Categorical Crossentropy as loss, because It is a classification task and as metrics the F1Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6Vk5hiGblaG"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=tfa.metrics.F1Score(num_classes = 7, average='macro')\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtuJ-reIh3bZ"
      },
      "source": [
        "I used callbacks in order to stop the training when the model goes in overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYphdNu0bo6k"
      },
      "outputs": [],
      "source": [
        "my_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=4),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngtENlc1bpXf",
        "outputId": "f1081173-723d-4fe6-ce1d-d1605aab39ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1357/1357 [==============================] - 56s 37ms/step - loss: 1.2126 - f1_score: 0.3264 - val_loss: 1.0840 - val_f1_score: 0.4337\n",
            "Epoch 2/20\n",
            "1357/1357 [==============================] - 48s 36ms/step - loss: 1.0648 - f1_score: 0.4784 - val_loss: 1.0083 - val_f1_score: 0.5454\n",
            "Epoch 3/20\n",
            "1357/1357 [==============================] - 48s 35ms/step - loss: 1.0250 - f1_score: 0.5143 - val_loss: 1.0107 - val_f1_score: 0.5309\n",
            "Epoch 4/20\n",
            "1357/1357 [==============================] - 51s 37ms/step - loss: 1.0018 - f1_score: 0.5372 - val_loss: 0.9891 - val_f1_score: 0.5542\n",
            "Epoch 5/20\n",
            "1357/1357 [==============================] - 49s 36ms/step - loss: 0.9843 - f1_score: 0.5499 - val_loss: 0.9925 - val_f1_score: 0.5650\n",
            "Epoch 6/20\n",
            "1357/1357 [==============================] - 48s 36ms/step - loss: 0.9676 - f1_score: 0.5539 - val_loss: 0.9855 - val_f1_score: 0.5733\n",
            "Epoch 7/20\n",
            "1357/1357 [==============================] - 48s 36ms/step - loss: 0.9575 - f1_score: 0.5641 - val_loss: 0.9736 - val_f1_score: 0.5725\n",
            "Epoch 8/20\n",
            "1357/1357 [==============================] - 48s 36ms/step - loss: 0.9474 - f1_score: 0.5655 - val_loss: 0.9790 - val_f1_score: 0.5725\n",
            "Epoch 9/20\n",
            "1357/1357 [==============================] - 48s 35ms/step - loss: 0.9374 - f1_score: 0.5750 - val_loss: 0.9637 - val_f1_score: 0.5754\n",
            "Epoch 10/20\n",
            "1357/1357 [==============================] - 48s 36ms/step - loss: 0.9294 - f1_score: 0.5766 - val_loss: 0.9719 - val_f1_score: 0.5761\n",
            "Epoch 11/20\n",
            "1357/1357 [==============================] - 48s 35ms/step - loss: 0.9217 - f1_score: 0.5806 - val_loss: 0.9666 - val_f1_score: 0.5686\n",
            "Epoch 12/20\n",
            "1357/1357 [==============================] - 48s 35ms/step - loss: 0.9111 - f1_score: 0.5835 - val_loss: 0.9712 - val_f1_score: 0.5667\n",
            "Epoch 13/20\n",
            "1357/1357 [==============================] - 48s 35ms/step - loss: 0.9080 - f1_score: 0.5821 - val_loss: 0.9716 - val_f1_score: 0.5807\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7883dd5290>"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x_train,y_train,epochs=20,batch_size=32,validation_data=(x_val,y_val),callbacks=my_callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eat81UBNGqfl"
      },
      "source": [
        "## Evaluation of the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7nBTFAIjKhA"
      },
      "source": [
        "For the evaluation, as I sai before, I used as reference point the F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVXWXnVUNZ7_",
        "outputId": "a6c7f726-cfd4-4291-dd4a-a822fa1d0989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.48      0.44       501\n",
            "           1       0.42      0.61      0.50        80\n",
            "           2       0.56      0.83      0.67        54\n",
            "           3       0.78      0.77      0.78      2000\n",
            "           4       0.66      0.59      0.63      1838\n",
            "           5       0.51      0.62      0.56       293\n",
            "           6       0.57      0.59      0.58       661\n",
            "\n",
            "    accuracy                           0.65      5427\n",
            "   macro avg       0.56      0.64      0.59      5427\n",
            "weighted avg       0.66      0.65      0.65      5427\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_pred=np.argmax(model.predict(x_test),axis=1)\n",
        "y_true=np.argmax(y_test,axis=1)\n",
        "\n",
        "print(metrics.classification_report(y_pred,y_true))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbsX81cWlHMW",
        "outputId": "31e66605-99eb-4f74-854c-fa84ad1c3919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5929419705723287\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "print(f1_score(y_true, y_pred, average='macro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUgqPAegjdgn"
      },
      "source": [
        "## Final result\n",
        "### With this configuration I reached a F1 Score equals to 0,5929"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zdoSd6qZiwc"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "togzUbG7ZoJg"
      },
      "source": [
        "[1]  Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n",
        "\n",
        "[2]  Polignano, M., Basile, P., de Gemmis, M., & Semeraro, G. (2019, June). A comparison of word-embeddings in emotion detection from text using bilstm, cnn and self-attention. In Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization (pp. 63-68)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CfkjBgNOQthp",
        "f3ugQlMj64oW",
        "letEhfzgRQt7",
        "79GF1kpPDWpH",
        "er6W-1NDEavI"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}